{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Q-learning for 200 steps with a deterministic greedy behavior policy: at each state $s_t$ use the best action $a_t \\in \\argmax_a Q(s_t,a)$ indicated by the current action-value table. If there is a tie, prefer move. Show the action-value table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'stay': 0, 'move': 0.0}, 'B': {'stay': 0, 'move': 0.0}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "states = ['A', 'B']\n",
    "actions = ['stay', 'move']\n",
    "gamma = 0.8  # discount factor\n",
    "alpha = 0.5  # learning rate\n",
    "num_steps = 200\n",
    "\n",
    "# Initialize the Q-table\n",
    "Q = {state: {action: 0 for action in actions} for state in states}\n",
    "\n",
    "# Function to choose the best action according to the deterministic greedy policy\n",
    "def choose_action(state, Q):\n",
    "    # If there is a tie, prefer 'move'\n",
    "    if Q[state]['stay'] == Q[state]['move']:\n",
    "        return 'move'\n",
    "    # Otherwise, choose the action with the highest Q-value\n",
    "    return max(Q[state], key=Q[state].get)\n",
    "\n",
    "# Function to get the next state based on the action\n",
    "def get_next_state(state, action):\n",
    "    if action == 'move':\n",
    "        return 'B' if state == 'A' else 'A'\n",
    "    return state  # stay in the same state\n",
    "\n",
    "# Q-learning process\n",
    "current_state = 'A'\n",
    "for _ in range(num_steps):\n",
    "    action = choose_action(current_state, Q)\n",
    "    next_state = get_next_state(current_state, action)\n",
    "    reward = 1 if action == 'stay' else 0\n",
    "    best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
    "    Q[current_state][action] = (1 - alpha) * Q[current_state][action] + \\\n",
    "                                alpha * (reward + gamma * Q[next_state][best_next_action])\n",
    "    current_state = next_state\n",
    "\n",
    "Q  # Display the final Q-table after 200 steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with $\\epsilon$-greedy policy with $\\epsilon=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'stay': 4.998335051817343, 'move': 3.9951919513688017},\n",
       " 'B': {'stay': 4.995702477721416, 'move': 3.994470571825331}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Parameters\n",
    "epsilon = 0.5  # Probability of exploring\n",
    "\n",
    "# Reinitialize the Q-table\n",
    "Q = {state: {action: 0 for action in actions} for state in states}\n",
    "\n",
    "# Function to choose an action according to the epsilon-greedy policy\n",
    "def epsilon_greedy_action(state, Q, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        # Explore: choose randomly between 'move' and 'stay'\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        # Exploit: choose the best action based on the current Q-table, break ties arbitrarily\n",
    "        best_actions = [action for action in actions if Q[state][action] == max(Q[state].values())]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "# Q-learning process with epsilon-greedy policy\n",
    "current_state = 'A'\n",
    "for _ in range(num_steps):\n",
    "    action = epsilon_greedy_action(current_state, Q, epsilon)\n",
    "    next_state = get_next_state(current_state, action)\n",
    "    reward = 1 if action == 'stay' else 0\n",
    "    best_next_action = max(Q[next_state], key=Q[next_state].get)\n",
    "    Q[current_state][action] = (1 - alpha) * Q[current_state][action] + \\\n",
    "                                alpha * (reward + gamma * Q[next_state][best_next_action])\n",
    "    current_state = next_state\n",
    "\n",
    "Q  # Display the final Q-table after 200 steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
