{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Identification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Estimate the class conditional probability (multinomial parameter) for English $$\\theta_{i,e} := \\hat p(c_i \\mid y=e)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.13212646167171935, 'b': 0.00992637505413599, 'c': 0.005214378518839324, 'd': 0.016370723256821134, 'e': 0.059991338241663056, 'f': 0.0034820268514508443, 'g': 0.014880900822867043, 'h': 0.031476829796448676, 'i': 0.09896925075790386, 'j': 0.0020961455175400605, 'k': 0.05718492854049372, 'l': 0.001195322650498051, 'm': 0.04083152880034647, 'n': 0.056942399307059334, 'o': 0.090376786487657, 'p': 0.0007102641836292768, 'q': 5.1970550021654394e-05, 'r': 0.04259852750108272, 's': 0.04259852750108272, 't': 0.05798181030749242, 'u': 0.07028150714595063, 'v': 0.00019055868341273278, 'w': 0.020285838025119098, 'x': 1.7323516673884798e-05, 'y': 0.013841489822433954, 'z': 0.007743611953226505, ' ': 0.12263317453443048}\n",
      "{'a': 0.10653656355302873, 'b': 0.009598032483191465, 'c': 0.03674720922433205, 'd': 0.040484213552230225, 'e': 0.11234968139642589, 'f': 0.007362217928038712, 'g': 0.007202516888384944, 'h': 0.0047750610856476675, 'i': 0.05000239551559481, 'j': 0.006723413769423639, 'k': 0.00023955155948065222, 'l': 0.05313253589280866, 'm': 0.02470575083443793, 'n': 0.0543462637941773, 'o': 0.07076353067058466, 'p': 0.02416276729961512, 'q': 0.007745500423207755, 'r': 0.058530431033106026, 's': 0.06635578197614067, 't': 0.03534184007537889, 'u': 0.034990497788140604, 'v': 0.005924908571154798, 'w': 0.00027149176741140586, 'x': 0.002571186738425667, 'y': 0.007394158135969465, 'z': 0.0033058115208330005, ' ': 0.16843668652282925}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "extracted_subdir_path = r\"C:\\\\Users\\\\Nicor\\\\OneDrive\\\\Documents\\\\Sem 9\\\\ECE 760\\\\HW4\\\\languageID\\\\languageID\"\n",
    "extracted_files = os.listdir(extracted_subdir_path)\n",
    "extracted_files.sort()\n",
    "\n",
    "# Function to calculate the class conditional probabilities with additive smoothing\n",
    "def calculate_class_conditional_probabilities(files, class_label, alpha, vocabulary_size, path):\n",
    "    character_counts = Counter()\n",
    "    total_characters = 0\n",
    "\n",
    "    # Process each file\n",
    "    for file in files:\n",
    "        # Filter by class label and training set (0.txt to 9.txt)\n",
    "        if file[0] == class_label and file[1] in '0123456789':\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                content = f.read().lower()\n",
    "                # Count characters ignoring non-alphabetic and non-space characters\n",
    "                filtered_content = ''.join(filter(lambda x: x in string.ascii_lowercase or x == ' ', content))\n",
    "                character_counts.update(filtered_content)\n",
    "                total_characters += len(filtered_content)\n",
    "\n",
    "    # Calculate probabilities\n",
    "    conditional_probabilities = {}\n",
    "    for char in string.ascii_lowercase + ' ':\n",
    "        conditional_probabilities[char] = (character_counts[char] + alpha) / (total_characters + alpha * vocabulary_size)\n",
    "\n",
    "    return conditional_probabilities\n",
    "\n",
    "# Vocabulary size (26 letters + space)\n",
    "vocabulary_size = 26 + 1\n",
    "alpha = 0.5  # smoothing parameter\n",
    "\n",
    "# Calculate class conditional probabilities for English\n",
    "theta_e = calculate_class_conditional_probabilities(extracted_files, 'e', alpha, vocabulary_size, extracted_subdir_path)\n",
    "theta_j = calculate_class_conditional_probabilities(extracted_files, 'j', alpha, vocabulary_size, extracted_subdir_path)\n",
    "theta_s = calculate_class_conditional_probabilities(extracted_files, 's', alpha, vocabulary_size, extracted_subdir_path)\n",
    "print(theta_j)\n",
    "print(theta_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Treat e10.txt as a test document $x$. Represent $x$ as a bag-of-words count vector. Print the bag-of-words vector $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[164,\n",
       " 32,\n",
       " 53,\n",
       " 57,\n",
       " 311,\n",
       " 55,\n",
       " 51,\n",
       " 140,\n",
       " 140,\n",
       " 3,\n",
       " 6,\n",
       " 85,\n",
       " 64,\n",
       " 139,\n",
       " 182,\n",
       " 53,\n",
       " 3,\n",
       " 141,\n",
       " 186,\n",
       " 225,\n",
       " 65,\n",
       " 31,\n",
       " 47,\n",
       " 4,\n",
       " 38,\n",
       " 2,\n",
       " 498]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to create a bag-of-words vector for a given document\n",
    "def create_bag_of_words_vector(file_path, vocabulary):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().lower()\n",
    "        # Count characters ignoring non-alphabetic and non-space characters\n",
    "        filtered_content = ''.join(filter(lambda x: x in string.ascii_lowercase or x == ' ', content))\n",
    "    \n",
    "    character_counts = Counter(filtered_content)\n",
    "\n",
    "    # Create vector based on the vocabulary\n",
    "    vector = [character_counts[char] for char in vocabulary]\n",
    "    return vector\n",
    "\n",
    "# Vocabulary (a to z and space)\n",
    "vocabulary = string.ascii_lowercase + ' '\n",
    "\n",
    "# Create the bag-of-words vector for e10.txt\n",
    "file_path = os.path.join(extracted_subdir_path, 'e10.txt')\n",
    "bag_of_words_vector_e10 = create_bag_of_words_vector(file_path, vocabulary)\n",
    "bag_of_words_vector_e10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Compute $\\hat p(x \\mid y)$ for $y=e, j, s$ under the multinomial model assumption, respectively. Use the formula $$\\hat p(x \\mid y) = \\prod_{i=1}^d \\theta_{i, y}^{x_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7841.662478537944, -8818.782947292133, -8421.593490399442)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function to compute log probability of x given y\n",
    "import math\n",
    "\n",
    "def compute_log_probability_x_given_y(bag_of_words_vector, theta_y):\n",
    "    log_prob = sum([count * math.log(theta_y[char]) for count, char in zip(bag_of_words_vector, string.ascii_lowercase + ' ')])\n",
    "    return log_prob\n",
    "\n",
    "# Now compute the log probabilities\n",
    "log_p_x_given_e = compute_log_probability_x_given_y(bag_of_words_vector_e10, theta_e)\n",
    "log_p_x_given_j = compute_log_probability_x_given_y(bag_of_words_vector_e10, theta_j)\n",
    "log_p_x_given_s = compute_log_probability_x_given_y(bag_of_words_vector_e10, theta_s)\n",
    "\n",
    "log_p_x_given_e, log_p_x_given_j, log_p_x_given_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Use Bayes rule and your estimated prior and likelihood, compute the posterior $\\hat p(y \\mid x)$. Show the three values: $\\hat p(y=e \\mid x), \\hat p(y=j \\mid x), \\hat p(y=s \\mid x)$. Show the predicted class label of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'e': 1.0, 'j': 0.0, 's': 1.3777222237663097e-252}, 'e')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bayes rule: p(y|x) = p(x|y)*p(y)/p(x) \n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Re-defining prior probabilities and log probabilities\n",
    "prior_probabilities = {'e': 0.333, 'j': 0.333, 's': 0.333}\n",
    "\n",
    "# Log prior probabilities\n",
    "log_priors = {key: math.log(value) for key, value in prior_probabilities.items()}\n",
    "\n",
    "# Log of the normalizing factor with log-sum-exp trick\n",
    "# Find the maximum of the log probabilities (log_p_x_given_y + log_priors[y])\n",
    "max_log_prob = max(log_p_x_given_e + log_priors['e'], log_p_x_given_j + log_priors['j'], log_p_x_given_s + log_priors['s'])\n",
    "\n",
    "# Compute the log-sum-exp\n",
    "log_normalizing_factor = max_log_prob + math.log(\n",
    "    sum(math.exp(log_p_x_given_y + log_priors[y] - max_log_prob)\n",
    "        for log_p_x_given_y, y in zip([log_p_x_given_e, log_p_x_given_j, log_p_x_given_s], ['e', 'j', 's']))\n",
    ")\n",
    "\n",
    "# Recompute the log posterior probabilities\n",
    "log_p_y_e_given_x = log_p_x_given_e + log_priors['e'] - log_normalizing_factor\n",
    "log_p_y_j_given_x = log_p_x_given_j + log_priors['j'] - log_normalizing_factor\n",
    "log_p_y_s_given_x = log_p_x_given_s + log_priors['s'] - log_normalizing_factor\n",
    "\n",
    "# Convert log posterior probabilities back to normal probabilities for easier interpretation\n",
    "p_y_e_given_x = math.exp(log_p_y_e_given_x)\n",
    "p_y_j_given_x = math.exp(log_p_y_j_given_x)\n",
    "p_y_s_given_x = math.exp(log_p_y_s_given_x)\n",
    "\n",
    "# Posterior probabilities and predicted class\n",
    "posterior_probabilities = {\n",
    "    'e': p_y_e_given_x,\n",
    "    'j': p_y_j_given_x,\n",
    "    's': p_y_s_given_x\n",
    "}\n",
    "predicted_class = max(posterior_probabilities, key=posterior_probabilities.get)\n",
    "\n",
    "posterior_probabilities, predicted_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Evaluate the performance of your classifier on the test set and create a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': {'e': 20, 'j': 0, 's': 0},\n",
       " 'j': {'e': 0, 'j': 20, 's': 0},\n",
       " 's': {'e': 0, 'j': 0, 's': 20}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict language and compute confusion matrix\n",
    "# Function to predict the class of a given document\n",
    "def predict_language(file_path, theta_e, theta_j, theta_s, priors, vocabulary):\n",
    "    # Create bag-of-words vector for the document\n",
    "    bag_of_words_vector = create_bag_of_words_vector(file_path, vocabulary)\n",
    "\n",
    "    # Compute log probabilities for each class\n",
    "    log_p_x_given_e = compute_log_probability_x_given_y(bag_of_words_vector, theta_e)\n",
    "    log_p_x_given_j = compute_log_probability_x_given_y(bag_of_words_vector, theta_j)\n",
    "    log_p_x_given_s = compute_log_probability_x_given_y(bag_of_words_vector, theta_s)\n",
    "\n",
    "    # Compute the log posterior probabilities using log-sum-exp trick\n",
    "    max_log_prob = max(log_p_x_given_e + log_priors['e'], log_p_x_given_j + log_priors['j'], log_p_x_given_s + log_priors['s'])\n",
    "    log_normalizing_factor = max_log_prob + math.log(\n",
    "        sum(math.exp(log_p_x_given_y + log_priors[y] - max_log_prob)\n",
    "            for log_p_x_given_y, y in zip([log_p_x_given_e, log_p_x_given_j, log_p_x_given_s], ['e', 'j', 's']))\n",
    "    )\n",
    "    \n",
    "    log_posterior_probabilities = {\n",
    "        'e': log_p_x_given_e + log_priors['e'] - log_normalizing_factor,\n",
    "        'j': log_p_x_given_j + log_priors['j'] - log_normalizing_factor,\n",
    "        's': log_p_x_given_s + log_priors['s'] - log_normalizing_factor\n",
    "    }\n",
    "    predicted_class = max(log_posterior_probabilities, key=log_posterior_probabilities.get)\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Confusion matrix setup\n",
    "confusion_matrix = {\n",
    "    'e': {'e': 0, 'j': 0, 's': 0},\n",
    "    'j': {'e': 0, 'j': 0, 's': 0},\n",
    "    's': {'e': 0, 'j': 0, 's': 0}\n",
    "}\n",
    "\n",
    "for file in extracted_files:\n",
    "    if file[1] in '10111213141516171819':\n",
    "        true_label = file[0]\n",
    "        file_path = os.path.join(extracted_subdir_path, file)\n",
    "        predicted_label = predict_language(file_path, theta_e, theta_j, theta_s, prior_probabilities, vocabulary)\n",
    "        confusion_matrix[predicted_label][true_label] += 1\n",
    "\n",
    "confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
